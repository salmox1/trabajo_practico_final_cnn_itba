{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TP Final - Style Transfer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCY6UbkkI9_N"
      },
      "source": [
        "# Style Transfer\n",
        "\n",
        "<img src=\"https://i0.wp.com/chelseatroy.com/wp-content/uploads/2018/12/neural_style_transfer.png?resize=768%2C311&ssl=1\">\n",
        "\n",
        "La idea de este trabajo final es reproducir el siguiente paper:\n",
        "\n",
        "https://arxiv.org/pdf/1508.06576.pdf\n",
        "\n",
        "El objetivo es transferir el estilo de una imagen dada a otra imagen distinta. \n",
        "\n",
        "Como hemos visto en clase, las primeras capas de una red convolucional se activan ante la presencia de ciertos patrones vinculados a detalles muy pequeños.\n",
        "\n",
        "A medida que avanzamos en las distintas capas de una red neuronal convolucional, los filtros se van activando a medida que detectan patrones de formas cada vez mas complejos.\n",
        "\n",
        "Lo que propone este paper es asignarle a la activación de las primeras capas de una red neuronal convolucional (por ejemplo VGG19) la definición del estilo y a la activación de las últimas capas de la red neuronal convolucional, la definición del contenido.\n",
        "\n",
        "La idea de este paper es, a partir de dos imágenes (una que aporte el estilo y otra que aporte el contenido) analizar cómo es la activación de las primeras capas para la imagen que aporta el estilo y cómo es la activación de las últimas capas de la red convolucional para la imagen que aporta el contenido. A partir de esto se intentará sintetizar una imagen que active los filtros de las primeras capas que se activaron con la imagen que aporta el estilo y los filtros de las últimas capas que se activaron con la imagen que aporta el contenido.\n",
        "\n",
        "A este procedimiento se lo denomina neural style transfer.\n",
        "\n",
        "# En este trabajo se deberá leer el paper mencionado y en base a ello, entender la implementación que se muestra a continuación y contestar preguntas sobre la misma.\n",
        "\n",
        "# Una metodología posible es hacer una lectura rápida del paper (aunque esto signifique no entender algunos detalles del mismo) y luego ir analizando el código y respondiendo las preguntas. A medida que se planteen las preguntas, volviendo a leer secciones específicas del paper terminará de entender los detalles que pudieran haber quedado pendientes.\n",
        "\n",
        "Lo primero que haremos es cargar dos imágenes, una que aporte el estilo y otra que aporte el contenido. A tal fin utilizaremos imágenes disponibles en la web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdiC5aWV9cWx",
        "outputId": "55a332e7-e42b-4104-d551-bdcbbc615bc7"
      },
      "source": [
        "!git clone https://github.com/salmox1/trabajo_practico_final_cnn_itba.git\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'trabajo_practico_final_cnn_itba' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxH20o2eFoc"
      },
      "source": [
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n",
        "from pathlib import Path\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLkV1bnFl_tK"
      },
      "source": [
        "# Definimos las imagenes que vamos a utilizar, y el directorio de salida\n",
        "\n",
        "base_image_path = \"trabajo_practico_final_cnn_itba/in/Neckarfront_Tubingen_Mai_2017.jpg\"\n",
        "#base_image_path = \"/content/drive/MyDrive/Diplomatura/in/foto.jpg\"\n",
        "style_reference_image_path = \"trabajo_practico_final_cnn_itba/in/La_noche_estrellada1.jpg\"\n",
        "#style_reference_image_path = \"/content/drive/MyDrive/Diplomatura/in/picasso.jpg\"\n",
        "result_prefix = \"trabajo_practico_final_cnn_itba/out/\"\n",
        "iterations = 2"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz2PeGfpeYzj"
      },
      "source": [
        "# 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?\n",
        "\n",
        "Respuesta:\n",
        "\n",
        "El paper renderiza una nueva imagen extrayendo información del contenido y del estilo de otras 2 imágenes proporcionadas.\n",
        "Para hacer esto aplica gradient descent teniendo en cuenta la siguiente función de Loss:\n",
        "\n",
        "$\\mathit{L}_{total}(\\vec{p},\\vec{a},\\vec{x}) = α  L_{content}(\\vec{p},\\vec{x}) + β L_{style}(\\vec{a},\\vec{x})$\n",
        "\n",
        "Donde:\n",
        "\n",
        "$\\vec{p}$: Imagen de contenido\n",
        "\n",
        "$\\vec{a}$: Imagen de estilo\n",
        "\n",
        "$\\vec{x}$: Imagen a generar\n",
        "\n",
        "$\\alpha$ (content_weight) y $\\beta$ (style_weight) funcionan como parametros de ajuste de la función de Loss, haciendo un trade off entre estilo y contenido (permite determinar si la imagen resultante va a tener más información del contenido en detrimento del estilo o viceversa)\n",
        "\n",
        "Por otro lado, en esta implementación además de las dos componentes nombradas más arriba se agrega un tercer componente que es el Total Variation Loss, una función de regularización que permite resolver problemas con de altas frecuencias en la imagen.\n",
        "\n",
        "Con lo que nuestra función de Loss quedaría\n",
        "\n",
        "$\\mathit{L}_{total}= \\alpha  L_{content} + \\beta L_{style} + \\gamma L_{total\\_variation\\_loss}$\n",
        "\n",
        "El siendo $\\gamma$ (total_variation_weight) un parámetro para dar una mayor o menor importancia a dicha función de regularización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Dt3aaEmJWS"
      },
      "source": [
        "total_variation_weight = 0.1\n",
        "style_weight = 10\n",
        "content_weight = 1"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7e-aDfD7YZQ",
        "outputId": "0264af33-3826-449b-f81c-3f4bd44f1f1c"
      },
      "source": [
        "load_img(style_reference_image_path).size"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(745, 596)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQJOhCVuse6"
      },
      "source": [
        "# Definimos el tamaño de las imágenes a utilizar\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2ct-8agm1E"
      },
      "source": [
        "# 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?\n",
        "\n",
        "Ayuda: https://keras.io/applications/\n",
        "\n",
        "Respuesta:\n",
        "\n",
        "Se define una función que lee la imagen y la formatea de la manera requerida.\n",
        "\n",
        "Primero se carga la imagen y se la transforma a un array, luego con np.expand_dims se le agrega una dimensión al array de la imagen, ya que como viene tiene es un tensor 3d con shape (height, width, depth) y keras requiere un tensor 4d con el batch_size (o cantidad de imágenes, en nuestro caso 1) además de las otras 3 dimensiones, el metodo expand_dims de numpy agrega esta dimensión extra quedando la imagen con un shape (bach_size, height, width, depth).\n",
        "\n",
        "Por otro lado, vgg19.preprocess_input formatea la imagen como requiere el modelo vgg19. \n",
        "\n",
        "Basicamente lo que hace es transformar la imagen de RGB a BGR y luego cada canal de color es zero centred con respecto al dataset de imageNet sin escalar. Esto se hace porque la red vgg19 fue entrenada con imágenes de ese formato y dado q vamos a reutilizar los pesos de ese modelo, necesitamos q nuestra imagen tenga el formato que entiende vgg19\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAkljg4zuzYd"
      },
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTf0YDSagt10"
      },
      "source": [
        "# 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?\n",
        "\n",
        "Respuesta:\n",
        "\n",
        "Hace exactamente el proceso inverso anterior, para luego poder visualizar la imagen resultante.\n",
        "\n",
        "Lo primero que es eliminar la dimensión extra del batch size haciendo un reshape de la imagen, luego elimina el zero centering sumando a cada canal de color la media de cada canal del dataset de imageNet, luego transforma el BGR a RGB y por ultimo deja cada uno de los pixels con intensidad entre 0 y 255.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5LaTrsAu14z"
      },
      "source": [
        "def deprocess_image(x):\n",
        "    x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNio09mu4S3"
      },
      "source": [
        "# get tensor representations of our images\n",
        "# K.variable convierte un numpy array en un tensor, para \n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Lbw02Uu--o"
      },
      "source": [
        "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJEi0YI3Uzrm"
      },
      "source": [
        "Aclaración:\n",
        "\n",
        "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGO_jGFfvEbF"
      },
      "source": [
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdG59VRavHGB",
        "outputId": "7aebc97d-7e20-4ba0-b417-54da98d1c944"
      },
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70-vs_jZkKVc"
      },
      "source": [
        "# 4) En la siguientes celdas:\n",
        "\n",
        "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
        "- ¿Por qué se permutan las dimensiones de x?\n",
        "\n",
        "La matriz de Gram para un conjunto de vectores { $\\vec v_{1}$, $\\vec v_{2}$,...,$\\vec v_{n}$} es aquella cuyos elementos están formados por el producto punto de cada uno de los vectores:\n",
        "\n",
        "$G_{ij}$ = <$\\vec v_{1}$,$\\vec v_{2}$>\n",
        "\n",
        "\n",
        "En este caso en particular, la matriz de Gram es utilizada para calcular la similitud del estilo. \n",
        "\n",
        "Una idea intuitiva del estilo es la correlación entre los diferentes filtros que se le aplica a la imagen, por poner un ejemplo en una foto del cuadro la noche estrellada de Van Gogh se esperaría que el filtro que detecta círculos se active en el mismo sector de la imagen donde se activa el filtro que detecta el color amarillo.\n",
        "\n",
        "Dado que el producto punto entre dos vectores no es otra cosa que la distancia coseno entre los mismos (lo que da una idea de similitud o cercanía de dichos vectores), la matriz de Gran tiene en cada uno de sus elementos dicha distancia coseno entre las activaciones de cada uno de los filtros:\n",
        "\n",
        "$G^l_{ij}$ = $\\sum_{(k)} F^l_{ik} F^l_{jk}$\n",
        "\n",
        "Donde $F^l_{ik}$ es la activación del filtro i de la capa l, en la porción k de la imagen y $F^l_{jk}$ es la activación del filtro j de la capa l, en la porción k de la imagen.\n",
        "\n",
        "La fución de costo que trata de capturar el estilo de la foto, quiere minimizar mide la distancia entre la matriz de Gram de la imagen de estilo y la matriz de Gram de la imagen output.\n",
        "\n",
        "Es necesario transponer las dimensiones para poder realizar el producto vectorial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1FODPATvJ1k"
      },
      "source": [
        "def gram_matrix(x):\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBQkKFY0Rbx-"
      },
      "source": [
        "# 5) Losses:\n",
        "\n",
        "Explicar qué mide cada una de las losses en las siguientes tres celdas.\n",
        "\n",
        "Rta:\n",
        "\n",
        "###Style Loss:\n",
        "\n",
        "Se calculan las matrices Gram de la imagen que estamos generando y de la imagen de estilo y luego se calcula el MSE entre ambas matrices.\n",
        "\n",
        "Cada capa contribuye a la loss total de la siguiente manera:\n",
        "\n",
        "    \n",
        "$E_{l} = \\frac{1}{4 N_{l}^{2} M_{l}^{2}} \\sum_{i,j} (G_{ij}^{l} - A_{ij}^{l})^2$\n",
        "\n",
        "Donde:\n",
        "\n",
        "$A^{l}$ : Representación del estilo de la imagen de estilo en la capa l\n",
        "    \n",
        "$G^{l}$ : Representación del estilo de la imagen que se está generando en la capa l\n",
        "\n",
        "$N_{l}$ : Cantidad de filtros\n",
        "\n",
        "$M_{l}$ : Tamaño del feature map\n",
        "\n",
        "Luego la Loss de estilo es:\n",
        "\n",
        "$L_{style}(\\vec{a},\\vec{x}) = \\sum \\limits _{l} ^{L} w_{l} E_{l}$\n",
        "\n",
        "donde $w_{l}$ el el peso relativo que se le da a cada capa que contribuye a estilo.\n",
        "\n",
        "Lo que quiere mostrar esta loss es la distancia entre las matrices de gram de la imagen de estilo y la imagen que estamos generando. Como ya vimos, las matrices de gram nos dan una especie de correlación entre los filtros, esta correlación es lo que nosotros llamamos estilo, entonces minimizando la distancia entre ambas matrices lo q en verdad hacemos es lograr que ambas imagenes tengan el mismo estilo. Esto se hace principalmente en las primeras capas de la red donde están los filtros q capturan texturas y colores pero no tanto las formas\n",
        "\n",
        "###Content Loss:\n",
        "\n",
        "La función de pérdida de contenido viene dada por:\n",
        "$L_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2} \\sum \\limits _{i,j} (F_{ij}^{l} - P_{ij}^{l})^2$\n",
        "\n",
        "Donde\n",
        " \n",
        "$\\vec{p}$: Imagen de contenido\n",
        "\n",
        "$\\vec{x}$: Imagen que se está generando\n",
        "\n",
        "$P^l_{ij}$: El feature map de la imagen de contenido en la capa l\n",
        "\n",
        "$F^l_{ij}$: El feature map de la imagen que se está generando\n",
        "\n",
        "De esta manera la Loss no es otra cosa que el MSE del feature map de ambas imágenes.\n",
        "\n",
        "En este caso lo que hace esta loss es minimizar la distancia de los diferentes features maps de la imagen de contenido y de la imagen que estamos generando. Haciendolo en las capaz más alejadas de la red podremos capturar el contenido de la imagen de contenido\n",
        "\n",
        "###Total Variation Loss\n",
        "\n",
        "Esta función de costo no está definida en el paper sino que se utiliza la implementación dada en https://keras.io/examples/generative/neural_style_transfer/\n",
        "\n",
        "Este componente de la función de costo es un término de regularización, dado que la suma ponderada de la loss de contenido y de la loss de estilo, genera muchos problemas en sectores de la imagen con frecuencias altas, sobre todo en los bordes.\n",
        "(https://www.tensorflow.org/tutorials/generative/style_transfer#total_variation_loss)\n",
        "\n",
        "\n",
        "Combinando todas estas y agregando los pesos vistos en el punto 1, tenemos nuestra función de costo final\n",
        "\n",
        "$\\mathit{L}_{total}= \\alpha  L_{content} + \\beta L_{style} + \\gamma L_{total\\_variation\\_loss}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Gt0ahWvN6q"
      },
      "source": [
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCqnju5RvQCo"
      },
      "source": [
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udEp5h31vRnY"
      },
      "source": [
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    a = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "    b = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-65vcinbvTZ0"
      },
      "source": [
        "# Armamos la loss total\n",
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss = loss + content_weight * content_loss(base_image_features,\n",
        "                                            combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :] \n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
        "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbz4n1OhvV2K"
      },
      "source": [
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JbydbOaVcvU"
      },
      "source": [
        "# 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?\n",
        "\n",
        "Respuesta:\n",
        "\n",
        "La función fmin_l_bfgs_b es una función optimizadora que utiliza el algoritmo L-BFGS-B para optimizar la función de Loss.\n",
        "\n",
        "Los parámetros que requiere son la función que se quiere optimizar y el gradiente de la misma, utilizando este optimizador no es necesaria la matriz hesiana de la función, con el gradiente es suficiente. Ambos parametros son calculados en clase evaluator que a su vez llama a la función eval_loss_and_grads q es la que los genera.\n",
        "\n",
        "En el paper no se nombra ninguna función optimizadora en particular por lo tanto se podría hacer utilizando cualquiera de los optimizadores que vimos en clase como SGD o Adam.\n",
        "\n",
        "Una observación es que en esta implementación no se parte de una imagen con ruido blanco inicializada aleatoriamente como nombra en el paper, sino que parte de la imagen de contenido y a partir de ahi es que va ajustando la imagen dependiendo la Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVE1_qemvZeN"
      },
      "source": [
        "def eval_loss_and_grads(x):\n",
        "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbl9roIgvdb1"
      },
      "source": [
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0yOEl-WOE6"
      },
      "source": [
        "# 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración.\n",
        "\n",
        "Comentario:\n",
        "\n",
        "Iteración a iteración se va marcando mucho más el estilo, sobre todo parecería ser que las sombras y las tonalidades de la pinturas, van adquiriendo más notoriedad a medida que avanzan las iteraciones.\n",
        "Las formas de las pinceladas del impresionismo es de las primeras indicios de estilo que el modelo logra transferir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n31YBwCVvhAI",
        "outputId": "94964df6-c724-4ac6-ccb6-9980d5c471ab"
      },
      "source": [
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix + '/' + ('output_at_iteration_%d.png' % i)\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
        "    print(load_img(fname, target_size=(img_nrows, img_ncols)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of iteration 0\n",
            "Current loss value: 13231204000.0\n",
            "Image saved as trabajo_practico_final_cnn_itba/out//output_at_iteration_0.png\n",
            "Iteration 0 completed in 13s\n",
            "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=517x400 at 0x7F838012BB00>\n",
            "Start of iteration 1\n",
            "Current loss value: 6613748700.0\n",
            "Image saved as trabajo_practico_final_cnn_itba/out//output_at_iteration_1.png\n",
            "Iteration 1 completed in 5s\n",
            "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=517x400 at 0x7F83800B6048>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkiJtofbWWy1"
      },
      "source": [
        "# 8) Generar imágenes para distintas combinaciones de pesos de las losses. Explicar las diferencias. (Adjuntar las imágenes generadas como archivos separados.)\n",
        "\n",
        "Respuesta:\n",
        "\n",
        "# 9) Cambiar las imágenes de contenido y estilo por unas elegidas por usted. Adjuntar el resultado.\n",
        "\n",
        "Respuesta:"
      ]
    }
  ]
}